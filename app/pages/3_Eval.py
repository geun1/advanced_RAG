import os
import sys
from pathlib import Path
import streamlit as st

BASE_DIR = Path(__file__).resolve().parents[2]
if str(BASE_DIR) not in sys.path:
    sys.path.append(str(BASE_DIR))

from src.config import settings
from src.modules.embeddings import OpenAIEmbeddings
from src.modules.vectorstore import ChromaVectorStore
from src.modules.retriever import SimpleRetriever
from src.modules.llm import OpenAIChatLLM
from src.modules.evaluation import (
    parse_dataset,
    run_evaluation,
    to_json_report,
    DATASET_TEMPLATE_JSONL,
)


st.set_page_config(page_title="RAG Evaluation", page_icon="ğŸ“ˆ", layout="wide")
st.title("ğŸ“ˆ RAG ì„±ëŠ¥ í‰ê°€")

st.markdown("""
ë‹¤ìŒ í˜•ì‹(csv/json/jsonl)ì˜ í‰ê°€ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. ê° í•­ëª©ì—ëŠ” ìµœì†Œ `question`ì´ í•„ìš”í•©ë‹ˆë‹¤.
- `ground_truth_answer`: ì„ íƒ(ì •ë‹µ í…ìŠ¤íŠ¸) â€” ìˆìœ¼ë©´ ì˜ë¯¸ ìœ ì‚¬ë„(relevance) ê³„ì‚°
- `ground_truth_sources`: ì„ íƒ(ë¬¸ì„œ ì‹ë³„ì ëª©ë¡ ë˜ëŠ” ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡  êµ¬ë¶„ ë¬¸ìì—´) â€” Recall/MRR/Precision ê³„ì‚°ì— ì‚¬ìš©
""")

with st.expander("ë°ì´í„°ì…‹ í…œí”Œë¦¿ ë³´ê¸°"):
    st.code(DATASET_TEMPLATE_JSONL, language="json")


@st.cache_resource(show_spinner=False)
def get_components():
    embeddings = OpenAIEmbeddings()
    store = ChromaVectorStore(embeddings=embeddings)
    retriever = SimpleRetriever(store)
    llm = OpenAIChatLLM()
    return embeddings, retriever, llm


embeddings, retriever, llm = get_components()

col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    uploaded = st.file_uploader("í‰ê°€ ë°ì´í„°ì…‹ ì—…ë¡œë“œ (csv/json/jsonl)")
with col2:
    top_k = st.slider("Top-K", 1, 10, settings.top_k)
with col3:
    use_llm_judge = st.checkbox("LLM íŒì‚¬ ì‚¬ìš©(ê·¼ê±° ì¶©ì‹¤ë„)", value=False)

col4, col5 = st.columns([1, 1])
with col4:
    use_bleu_rouge = st.checkbox("BLEU/ROUGE ì‚¬ìš©", value=True)
with col5:
    use_bertscore = st.checkbox("BERTScore ì‚¬ìš©", value=False)

bert_model_type = "xlm-roberta-large"
if use_bertscore:
    bert_model_type = st.text_input("BERTScore ëª¨ë¸", value="xlm-roberta-large")

items = []
if uploaded is not None:
    try:
        # memoryviewê°€ ì•„ë‹Œ bytesë¡œ ì „ë‹¬
        items = parse_dataset(uploaded.getvalue(), uploaded.name)
        st.success(f"í•­ëª© {len(items)}ê°œ ë¡œë“œ")
    except Exception as e:
        st.error(f"ë°ì´í„°ì…‹ íŒŒì‹± ì‹¤íŒ¨: {e}")

run = st.button("í‰ê°€ ì‹¤í–‰")

if run:
    if not items:
        st.warning("í‰ê°€ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. í…œí”Œë¦¿ì„ ì°¸ê³ í•´ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ì„¸ìš”.")
    else:
        with st.spinner("í‰ê°€ ì¤‘..."):
            results, aggregate = run_evaluation(
                items=items,
                retriever=retriever,
                llm=llm,
                embeddings=embeddings,
                top_k=top_k,
                enable_llm_judge=use_llm_judge,
                enable_bleu_rouge=use_bleu_rouge,
                enable_bertscore=use_bertscore,
                bert_model_type=bert_model_type,
            )

        st.subheader("ì§‘ê³„ ê²°ê³¼")
        m1 = aggregate.retrieval_avg
        m2 = aggregate.generation_avg
        m3 = aggregate.latency_avg
        c1, c2, c3, c4 = st.columns(4)
        with c1:
            st.metric("Recall@k", f"{m1.recall_at_k:.3f}")
            st.metric("Precision@k", f"{m1.precision_at_k:.3f}")
            st.metric("MRR@k", f"{m1.mrr_at_k:.3f}")
        with c2:
            st.metric("Relevance(cos)", f"{(m2.relevance_cosine if m2.relevance_cosine is not None else 0.0):.3f}")
            if m2.bleu is not None:
                st.metric("BLEU", f"{m2.bleu:.3f}")
            if m2.rouge_l is not None:
                st.metric("ROUGE-L", f"{m2.rouge_l:.3f}")
            if m2.bertscore_f1 is not None:
                st.metric("BERTScore-F1", f"{m2.bertscore_f1:.3f}")
        with c3:
            st.metric("End-to-End(ms)", f"{m3.end_to_end_ms:.1f}")
            st.metric("Retriever(ms)", f"{m3.retriever_ms:.1f}")
            st.metric("Generator(ms)", f"{m3.generator_ms:.1f}")
        with c4:
            st.metric("Avg Answer Len", f"{m2.answer_length}")
            st.metric("Avg GT Len", f"{m2.ground_truth_length}")

        st.subheader("ê°œë³„ ê²°ê³¼")
        for i, r in enumerate(results, start=1):
            with st.expander(f"[{i}] {r.question}"):
                st.markdown("**Answer**")
                st.write(r.answer)
                st.markdown("**Retrieved Sources**")
                st.write(r.retrieved_sources)
                st.markdown("**Retrieval**")
                st.json({
                    "recall@k": r.retrieval.recall_at_k,
                    "precision@k": r.retrieval.precision_at_k,
                    "mrr@k": r.retrieval.mrr_at_k,
                })
                st.markdown("**Generation**")
                st.json({
                    "relevance_cosine": r.generation.relevance_cosine,
                    "bleu": r.generation.bleu,
                    "rouge_l": r.generation.rouge_l,
                    "bertscore_f1": r.generation.bertscore_f1,
                    "faithfulness_judgement": r.generation.faithfulness_judgement,
                    "answer_length": r.generation.answer_length,
                })
                st.markdown("**Latency (ms)**")
                st.json({
                    "e2e": r.latency.end_to_end_ms,
                    "retriever": r.latency.retriever_ms,
                    "generator": r.latency.generator_ms,
                })

        st.subheader("ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°")
        report = to_json_report(results, aggregate)
        st.download_button(
            label="JSON ë‹¤ìš´ë¡œë“œ",
            data=report.encode("utf-8"),
            file_name="rag_eval_report.json",
            mime="application/json",
        )


